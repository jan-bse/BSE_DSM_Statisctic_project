bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p + 2*log(choose(ncol(x),p))
}
sel <- which.min(bic)
beta <- c(fit$a0[sel],fit$beta[,sel]); names(beta)[1]= 'Intercept'
ypred <- pred[,sel]
ans <- list(coef=beta,ypred=ypred,lambda.opt=fit$lambda[sel],lambda=data.frame(lambda=fit$lambda,bic=bic,nvars=p))
return(ans)
}
result <- lasso.bic(X, y)
gaussian_log_likelihood <- function(mu, y) {
n <- length(y)
residuals <- y - mu
sigma_sq <- sum(residuals^2) / (n - length(mu))
log_likelihood <- -0.5 * (n * log(2 * pi * sigma_sq) + sum(residuals^2) / sigma_sq)
print(log_likelihood)
return(log_likelihood)
}
lasso.bic <- function(x,y,extended=FALSE) {
#Select model in LASSO path with best BIC (using LASSO regression estimates)
#Input
# - y: vector with response variable
# - x: design matrix
# - extended: whether to use EBIC (Chen and Chen 2008) instead of BIC
#
#Output: list with the following elements
# - coef: LASSO-estimated regression coefficient with lambda set via BIC
# - ypred: predicted y
# - lambda.opt: optimal value of lambda
# - lambda: data.frame with bic and number of selected variables for each value of lambda
print(dim(x))
print(dim(y))
require(glmnet)
fit <- glmnet(x=x,y=y,family='gaussian',alpha=1)
pred <- cbind(1,x) %*% rbind(fit$a0,fit$beta)
# print(pred)
n <- length(y)
p <- colSums(fit$beta!=0) + 1
if (!extended){
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p
} else {
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p + 2*log(choose(ncol(x),p))
}
sel <- which.min(bic)
beta <- c(fit$a0[sel],fit$beta[,sel]); names(beta)[1]= 'Intercept'
ypred <- pred[,sel]
ans <- list(coef=beta,ypred=ypred,lambda.opt=fit$lambda[sel],lambda=data.frame(lambda=fit$lambda,bic=bic,nvars=p))
return(ans)
}
result <- lasso.bic(X, y)
gaussian_log_likelihood <- function(predictions, y) {
n <- length(y)
k <- ncol(predictions)
residuals <- y - predictions
sigma_sq <- apply(residuals^2, 2, sum) / (n - k)
log_likelihood <- -0.5 * (n * log(2 * pi * sigma_sq) + colSums(residuals^2) / sigma_sq)
return(log_likelihood)
}
lasso.bic <- function(x,y,extended=FALSE) {
#Select model in LASSO path with best BIC (using LASSO regression estimates)
#Input
# - y: vector with response variable
# - x: design matrix
# - extended: whether to use EBIC (Chen and Chen 2008) instead of BIC
#
#Output: list with the following elements
# - coef: LASSO-estimated regression coefficient with lambda set via BIC
# - ypred: predicted y
# - lambda.opt: optimal value of lambda
# - lambda: data.frame with bic and number of selected variables for each value of lambda
print(dim(x))
print(dim(y))
require(glmnet)
fit <- glmnet(x=x,y=y,family='gaussian',alpha=1)
pred <- cbind(1,x) %*% rbind(fit$a0,fit$beta)
# print(pred)
n <- length(y)
p <- colSums(fit$beta!=0) + 1
if (!extended){
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p
} else {
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p + 2*log(choose(ncol(x),p))
}
sel <- which.min(bic)
beta <- c(fit$a0[sel],fit$beta[,sel]); names(beta)[1]= 'Intercept'
ypred <- pred[,sel]
ans <- list(coef=beta,ypred=ypred,lambda.opt=fit$lambda[sel],lambda=data.frame(lambda=fit$lambda,bic=bic,nvars=p))
return(ans)
}
result <- lasso.bic(X, y)
result
gaussian_log_likelihood <- function(predictions, y) {
n <- length(y)
k <- ncol(predictions)
residuals <- y - predictions
sigma_sq <- apply(residuals^2, 2, sum) / (n - k)
log_likelihood <- -0.5 * (n * log(2 * pi * sigma_sq) + colSums(residuals^2) / sigma_sq)
return(log_likelihood)
}
lasso.bic <- function(x,y,extended=FALSE) {
#Select model in LASSO path with best BIC (using LASSO regression estimates)
#Input
# - y: vector with response variable
# - x: design matrix
# - extended: whether to use EBIC (Chen and Chen 2008) instead of BIC
#
#Output: list with the following elements
# - coef: LASSO-estimated regression coefficient with lambda set via BIC
# - ypred: predicted y
# - lambda.opt: optimal value of lambda
# - lambda: data.frame with bic and number of selected variables for each value of lambda
print(dim(x))
print(y)
require(glmnet)
fit <- glmnet(x=x,y=y,family='gaussian',alpha=1)
pred <- cbind(1,x) %*% rbind(fit$a0,fit$beta)
# print(pred)
n <- length(y)
p <- colSums(fit$beta!=0) + 1
if (!extended){
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p
} else {
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p + 2*log(choose(ncol(x),p))
}
sel <- which.min(bic)
beta <- c(fit$a0[sel],fit$beta[,sel]); names(beta)[1]= 'Intercept'
ypred <- pred[,sel]
ans <- list(coef=beta,ypred=ypred,lambda.opt=fit$lambda[sel],lambda=data.frame(lambda=fit$lambda,bic=bic,nvars=p))
return(ans)
}
result <- lasso.bic(X, y)
result <- lasso.bic(X, y)
gaussian_log_likelihood <- function(predictions, y) {
n <- length(y)
k <- ncol(predictions)
residuals <- y - predictions
sigma_sq <- apply(residuals^2, 2, sum) / (n - k)
log_likelihood <- -0.5 * (n * log(2 * pi * sigma_sq) + colSums(residuals^2) / sigma_sq)
return(log_likelihood)
}
lasso.bic <- function(x,y,extended=FALSE) {
#Select model in LASSO path with best BIC (using LASSO regression estimates)
#Input
# - y: vector with response variable
# - x: design matrix
# - extended: whether to use EBIC (Chen and Chen 2008) instead of BIC
#
#Output: list with the following elements
# - coef: LASSO-estimated regression coefficient with lambda set via BIC
# - ypred: predicted y
# - lambda.opt: optimal value of lambda
# - lambda: data.frame with bic and number of selected variables for each value of lambda
print(dim(x))
require(glmnet)
fit <- glmnet(x=x,y=y,family='gaussian',alpha=1)
pred <- cbind(1,x) %*% rbind(fit$a0,fit$beta)
# print(pred)
n <- length(y)
p <- colSums(fit$beta!=0) + 1
if (!extended){
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p
} else {
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p + 2*log(choose(ncol(x),p))
}
sel <- which.min(bic)
beta <- c(fit$a0[sel],fit$beta[,sel]); names(beta)[1]= 'Intercept'
ypred <- pred[,sel]
ans <- list(coef=beta,ypred=ypred,lambda.opt=fit$lambda[sel],lambda=data.frame(lambda=fit$lambda,bic=bic,nvars=p))
return(ans)
}
result <- lasso.bic(X, y)
result
View(result)
gaussian_log_likelihood <- function(predictions, y) {
n <- length(y)
k <- ncol(predictions)
residuals <- y - predictions
sigma_sq <- apply(residuals^2, 2, sum) / (n - k)
log_likelihood <- -0.5 * (n * log(2 * pi * sigma_sq) + colSums(residuals^2) / sigma_sq)
return(log_likelihood)
}
lasso.bic <- function(x,y,extended=FALSE) {
#Select model in LASSO path with best BIC (using LASSO regression estimates)
#Input
# - y: vector with response variable
# - x: design matrix
# - extended: whether to use EBIC (Chen and Chen 2008) instead of BIC
#
#Output: list with the following elements
# - coef: LASSO-estimated regression coefficient with lambda set via BIC
# - ypred: predicted y
# - lambda.opt: optimal value of lambda
# - lambda: data.frame with bic and number of selected variables for each value of lambda
print(dim(x))
require(glmnet)
fit <- glmnet(x=x,y=y,family='gaussian',alpha=1)
pred <- cbind(1,x) %*% rbind(fit$a0,fit$beta)
# print(pred)
n <- length(y)
p <- colSums(fit$beta!=0) + 1
if (!extended){
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p
} else {
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p + 2*log(choose(ncol(x),p))
}
sel <- which.min(bic)
beta <- c(fit$a0[sel],fit$beta[,sel]); names(beta)[1]= 'Intercept'
ypred <- pred[,sel]
ans <- list(coef=beta,ypred=ypred,lambda.opt=fit$lambda[sel],lambda=data.frame(lambda=fit$lambda,bic=bic,nvars=p))
return(ans, sel, beta)
}
result <- lasso.bic(X, y)
gaussian_log_likelihood <- function(predictions, y) {
n <- length(y)
k <- ncol(predictions)
residuals <- y - predictions
sigma_sq <- apply(residuals^2, 2, sum) / (n - k)
log_likelihood <- -0.5 * (n * log(2 * pi * sigma_sq) + colSums(residuals^2) / sigma_sq)
return(log_likelihood)
}
lasso.bic <- function(x,y,extended=FALSE) {
#Select model in LASSO path with best BIC (using LASSO regression estimates)
#Input
# - y: vector with response variable
# - x: design matrix
# - extended: whether to use EBIC (Chen and Chen 2008) instead of BIC
#
#Output: list with the following elements
# - coef: LASSO-estimated regression coefficient with lambda set via BIC
# - ypred: predicted y
# - lambda.opt: optimal value of lambda
# - lambda: data.frame with bic and number of selected variables for each value of lambda
print(dim(x))
require(glmnet)
fit <- glmnet(x=x,y=y,family='gaussian',alpha=1)
pred <- cbind(1,x) %*% rbind(fit$a0,fit$beta)
# print(pred)
n <- length(y)
p <- colSums(fit$beta!=0) + 1
if (!extended){
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p
} else {
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p + 2*log(choose(ncol(x),p))
}
sel <- which.min(bic)
beta <- c(fit$a0[sel],fit$beta[,sel]); names(beta)[1]= 'Intercept'
ypred <- pred[,sel]
ans <- list(coef=beta,ypred=ypred,lambda.opt=fit$lambda[sel],lambda=data.frame(lambda=fit$lambda,bic=bic,nvars=p))
return(ans)
}
result$lambda
result <- lasso.bic(X, y)
result$lambda[order(result$lambda$BIC, decreasing = FALSE), ]
result$lambda[order(as.vector(result$lambda$BIC), decreasing = FALSE), ]
result$lambda[order(result$lambda$BIC), ]
result$lambda[order(result$lambda$bic), ]
result$coef
X_test %*% result$lambda[order(result$lambda$bic), ]
cbind(1,X_test) %*% result$lambda[order(result$lambda$bic), ]
result$lambda[order(result$lambda$bic), ]
cbind(1,X_test) %*% result$coef
result_BIC <- lasso.bic(X, y)
result_EBIC <- lasso.bic(X, y, True)
result_BIC <- lasso.bic(X, y)
result_EBIC <- lasso.bic(X, y, TRUE)
result_BIC$lambda[order(result$lambda$bic), ]
result_EBIC$lambda[order(result$lambda$bic), ]
test_pred_BIC <- cbind(1,X_test) %*% result_BIC$coef
# Create a data frame with the predictions
predictions_df <- data.frame(pred = test_pred_BIC)
# Write the data frame to a CSV file
write.csv(predictions_df, "predictions/lasso_BIC.csv", row.names = FALSE)
# Create a data frame with the predictions
predictions_df <- data.frame(pred = test_pred_BIC)
# Write the data frame to a CSV file
write.csv(predictions_df, "predictions/lasso_BIC.csv")
# Create a data frame with the predictions
predictions_df <- data.frame(pred = test_pred_BIC)
# Write the data frame to a CSV file
write.csv(predictions_df, "predictions/lasso_BIC.csv")
# Define a function to calculate scores for different 'g' values
calculate_scores <- function(y_f, x_f, g_range) {
# Initialize an empty dataframe to store results
result_df <- data.frame(G = numeric(0), Score = numeric(0))
# Loop through the range of 'g' values
for (g in g_range) {
# Fit the model with the current 'g' value
fit <- modelSelection(y=y_f ,x=x_f, priorCoef=zellnerprior(taustd=g), priorDelta=modelbbprior(1,1))
post.prob = postProb(fit)
# Extract the score (the first element in the 'pp' column)
score <- post.prob$pp[1]
# Append the 'g' and score to the result dataframe
result_df <- rbind(result_df, data.frame(G = g, Score = score))
}
return(result_df)
}
# Define a range of 'g' values
g_range <- seq(0.03, 1, by = 0.03)
# Call the function to calculate scores for different 'g' values
result_dataframe <- calculate_scores(y_f = y, x_f = X, g_range)
library(hdm)
install.packages("hdm")
library(hdm)
library(glmnet)
library(ggplot2)
library(tidyverse)
library(HDCI)
install.packages("HDCI")
library(hdm)
library(glmnet)
library(ggplot2)
library(tidyverse)
library(HDCI)
library(gridExtra)
install.packages("gridExtra")
install.packages("mombf")
install.packages("rstanarm")
library(hdm)
library(glmnet)
library(ggplot2)
library(tidyverse)
library(HDCI)
library(gridExtra)
library(mombf) # Bayesian model selection and Bayesian model averaging
library(rstanarm)
mc.cores = parallel::detectCores()
# Read the data
df <- read.csv('datasets/train_set.csv')
test_df <- read.csv('datasets/test_set.csv')
X <- df[, 4:ncol(df)]
y <- df$cpi_pct
X_test <- test_df[, 4:ncol(test_df)]
X <- as.matrix(X)
y <- as.vector(y)
X_test <- as.matrix(X_test)
# Define a function to calculate scores for different 'g' values
calculate_scores <- function(y_f, x_f, g_range) {
# Initialize an empty dataframe to store results
result_df <- data.frame(G = numeric(0), Score = numeric(0))
# Loop through the range of 'g' values
for (g in g_range) {
# Fit the model with the current 'g' value
fit <- modelSelection(y=y_f ,x=x_f, priorCoef=zellnerprior(taustd=g), priorDelta=modelbbprior(1,1))
post.prob = postProb(fit)
# Extract the score (the first element in the 'pp' column)
score <- post.prob$pp[1]
# Append the 'g' and score to the result dataframe
result_df <- rbind(result_df, data.frame(G = g, Score = score))
}
return(result_df)
}
# Define a range of 'g' values
g_range <- seq(0.03, 1, by = 0.03)
# Call the function to calculate scores for different 'g' values
result_dataframe <- calculate_scores(y_f = y, x_f = X, g_range)
# Define a range of 'g' values
g_range <- c(0.1, 1, 10, 100, 1000, 5000)
# g_range <- seq(0.03, 1, by = 0.03)
# Call the function to calculate scores for different 'g' values
result_dataframe <- calculate_scores(y_f = y, x_f = X, g_range)
print(head(result_dataframe))
ggplot(result_dataframe, aes(x = G, y = Score)) +
geom_point() +
labs(x = "g", y = "Best Model Proba") +
ggtitle("Best_Model_Probability vs. g")
View(result_dataframe)
View(result_dataframe)
# Define a range of 'g' values
g_range <- c(0.1, 0.2, 0.5, 1, 2)
# g_range <- seq(0.03, 1, by = 0.03)
# Call the function to calculate scores for different 'g' values
result_dataframe <- calculate_scores(y_f = y, x_f = X, g_range)
print(head(result_dataframe))
ggplot(result_dataframe, aes(x = G, y = Score)) +
geom_point() +
labs(x = "g", y = "Best Model Proba") +
ggtitle("Best_Model_Probability vs. g")
g = 0.5
margpp <- 0.5
fit.bayesreg <- modelSelection(y=y, x=x, priorCoef=zellnerprior(taustd = g), priorDelta=modelbbprior(1,1))
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = g), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
View(fit.bayesreg)
ci.bayesreg <- coef(fit.bayesreg)[-c(1,nrow(coef(fit.bayesreg))),]
coef(fit.bayesreg)
View(fit.bayesreg)
fit.bayesreg
predict(fit.bayesreg)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = g), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 5000), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 30), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 5), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 10), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 3), priorDelta=modelbbprior(1,1))
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 50), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 500), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 5000), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 50000), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 500), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 5000), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 1800), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 1800), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 500), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 1000), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 3000), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 5000), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 2500), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 2500), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 2500), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = 2500), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
ci.bayesreg <- coef(fit.bayesreg)[-c(1,nrow(coef(fit.bayesreg))),]
sel.bayesreg <- ci.bayesreg[,4] > 0.5
ci.bayesreg[,1:3] <- round(ci.bayesreg[,1:3], 3)
ci.bayesreg[,4] <- round(ci.bayesreg[,4], 4)
sorted_indices <- order(ci.bayesreg[, "margpp"], decreasing = TRUE)
# Sort the matrix by the specified column
ci.bayesreg <- ci.bayesreg[sorted_indices, ]
head(ci.bayesreg, 5)
tail(ci.bayesreg, 5)
sum(ci.bayesreg[, "margpp"] > margpp)
sum(ci.bayesreg[, "margpp"] > 0.3)
ci.bayesreg[, "margpp"]
predict(fit.bayesreg)
help(modelSelection)
postProb(fit.bayesreg)
postMode(fit.bayesreg)
postModeProb(fit.bayesreg)
margpp(fit.bayesreg)
postProb(fit.bayesreg)
fit1$margpp
fit.bayesreg$margpp
coef(fit.bayesreg)
betas = coef(fit.bayesreg)$estimate
coef(fit.bayesreg)
betas <- coef(fit.bayesreg)
class(betas)
betasa
betas
betas[, "estimate", drop = FALSE]
betas <- as.vector(coef(fit.bayesreg)[, "estimate", drop = FALSE])
betas
bma_betas <- as.vector(coef(fit.bayesreg)[, "estimate", drop = FALSE])
bma_pred <-cbind(1,X_test) %*% bma_betas
cbind(1,X_test)
bma_betas
result_BIC$coef
bma_betas <- coef(fit.bayesreg)[, "estimate", drop = FALSE]
bma_pred <-cbind(1,X_test) %*% bma_betas
bma_pred <-cbind(1,X_test) %*% bma_betas
bma_betas
class(result_BIC$coef)
class(bma_betas)
bma_betas <- as.numeric(coef(fit.bayesreg)[, "estimate", drop = FALSE])
bma_pred <-cbind(1,X_test) %*% bma_betas
bma_pred <-cbind(1,X_test) %*% bma_betas
bma_pred <-cbind(1,X_test) %*% bma_betas
bma_betas
print(dim(cbind(1, X_test)))
print(dim(bma_betas))
bma_betas <- coef(fit.bayesreg)[, "estimate", drop = FALSE]
print(dim(cbind(1, X_test)))
print(dim(bma_betas))
print(dim(X_test))
print(dim(cbind(1, X_test)))
print(dim(bma_betas))
bma_betas
bma_betas[1]
bma_betas[:-1]
predict(fit.bayesreg)
help(predict(fit.bayesreg))
bma_betas <- coef(fit.bayesreg)[, "estimate", drop = FALSE]
class(bma_betas)
bma_betas <- coef(fit.bayesreg)[, "estimate", drop = FALSE][-nrow(my_matrix), ]
bma_betas <- coef(fit.bayesreg)[, "estimate", drop = FALSE]
bma_betas <- [-nrow(bma_betas), ]
bma_betas <- coef(fit.bayesreg)[, "estimate", drop = FALSE]
bma_betas <- bma_betas[-nrow(bma_betas), ]
bma_betas
print(dim(X_test))
print(dim(cbind(1, X_test)))
print(dim(bma_betas))
bma_pred <-cbind(1,X_test) %*% bma_betas
bma_pred <-cbind(1,X_test) %*% bma_betas
bma_pred
bma_pred <-cbind(1,X_test) %*% bma_betas
# Create a data frame with the predictions
predictions_df <- data.frame(pred = test_pred_BMA)
test_pred_BMA <-cbind(1,X_test) %*% bma_betas
# Create a data frame with the predictions
predictions_df <- data.frame(pred = test_pred_BMA)
# Write the data frame to a CSV file
write.csv(predictions_df, "predictions/BMA_pred.csv")
