ypred <- pred[,sel]
ans <- list(coef=beta,ypred=ypred,lambda.opt=fit$lambda[sel],lambda=data.frame(lambda=fit$lambda,bic=bic,nvars=p))
return(ans)
}
result_BIC <- lasso.bic(X, y)
result_EBIC <- lasso.bic(X, y, TRUE)
result_BIC$lambda[order(result_BIC$lambda$bic), ]
result_EBIC$lambda[order(result_EBIC$lambda$bic), ]
test_pred_BIC <- cbind(1,X_test) %*% result_BIC$coef
test_pred_EBIC <- cbind(1,X_test) %*% result_EBIC$coef
# Create a data frame with the predictions
predictions_df <- data.frame(pred = test_pred_BIC)
# Write the data frame to a CSV file
write.csv(predictions_df, paste("predictions/", pca_prefix, "lasso_bic.csv", sep = ""))
# Create a data frame with the predictions
predictions_df <- data.frame(pred = test_pred_EBIC)
# Write the data frame to a CSV file
write.csv(predictions_df, paste("predictions/", pca_prefix, "lasso_ebic.csv", sep = ""))
library(hdm)
library(glmnet)
library(ggplot2)
library(tidyverse)
library(HDCI)
library(gridExtra)
library(mombf) # Bayesian model selection and Bayesian model averaging
library(rstanarm)
mc.cores = parallel::detectCores()
# Define a function to calculate scores for different 'g' values
calculate_scores <- function(y_f, x_f, g_range) {
# Initialize an empty dataframe to store results
result_df <- data.frame(G = numeric(0), Score = numeric(0))
# Loop through the range of 'g' values
for (g in g_range) {
# Fit the model with the current 'g' value
fit <- modelSelection(y=y_f ,x=x_f, priorCoef=zellnerprior(taustd=g), priorDelta=modelbbprior(1,1))
post.prob = postProb(fit)
# Extract the score (the first element in the 'pp' column)
score <- post.prob$pp[1]
# Append the 'g' and score to the result dataframe
result_df <- rbind(result_df, data.frame(G = g, Score = score))
}
return(result_df)
}
g_cv <- 1/8.697490026177834e-05
g_ebic <- 1/3.219938e-04
print(g_cv)
# Define a range of 'g' values
g_range <- c(5, 50, 100, 500, 1000, 1200, 1500, 2500, 3000, 5000, 10000)
# g_range <- seq(0.03, 1, by = 0.03)
# Call the function to calculate scores for different 'g' values
result_dataframe <- calculate_scores(y_f = y, x_f = X, g_range)
print(head(result_dataframe))
ggplot(result_dataframe, aes(x = G, y = Score)) +
geom_point() +
labs(x = "g", y = "Model Proba") +
ggtitle("Best Model_margpp vs. g")
g = 11000
margpp <- 0.5
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = g), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
ci.bayesreg <- coef(fit.bayesreg)[-c(1,nrow(coef(fit.bayesreg))),]
sel.bayesreg <- ci.bayesreg[,4] > 0.5
ci.bayesreg[,1:3] <- round(ci.bayesreg[,1:3], 3)
ci.bayesreg[,4] <- round(ci.bayesreg[,4], 4)
sorted_indices <- order(ci.bayesreg[, "margpp"], decreasing = TRUE)
# Sort the matrix by the specified column
ci.bayesreg <- ci.bayesreg[sorted_indices, ]
# Fill missing values with 0 using the fillna() function
ci.bayesreg <- ifelse(is.na(ci.bayesreg), 0, ci.bayesreg)
head(ci.bayesreg, 5)
tail(ci.bayesreg, 5)
sum(ci.bayesreg[, 'margpp'] > margpp)
bma_betas <- coef(fit.bayesreg)[, "estimate", drop = FALSE]
bma_betas <- bma_betas[-nrow(bma_betas), ]
print(dim(X_test))
print(dim(cbind(1, X_test)))
head(bma_betas)
test_pred_BMA <-cbind(1,X_test) %*% bma_betas
# Create a data frame with the predictions
predictions_df <- data.frame(pred = test_pred_BMA)
# Write the data frame to a CSV file
write.csv(predictions_df, paste("predictions/", pca_prefix, "bma.csv", sep = ""))
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
USE_PCA <- FALSE
library(glmnet)
library(caret)
library(data.table)
# Set the flag for PCA
USE_PCA <- TRUE  # Change this to FALSE if not using PCA
# Define PCA prefixes and suffixes
pca_prefix <- if (USE_PCA) 'pca_' else ''
pca_suffix <- if (USE_PCA) '_pca' else ''
pca_prefix
USE_PCA
knitr::opts_chunk$set(echo = TRUE)
# Set the flag for PCA
USE_PCA <- TRUE  # Change this to FALSE if not using PCA
library(glmnet)
library(caret)
library(data.table)
# Define PCA prefixes and suffixes
pca_prefix <- if (USE_PCA) 'pca_' else ''
pca_suffix <- if (USE_PCA) '_pca' else ''
# Read the CSV files
df <- fread(paste0('datasets/train_set', pca_suffix, '.csv'))
test_df <- fread(paste0('datasets/test_set', pca_suffix, '.csv'))
# Subset the data based on PCA flag
if (USE_PCA) {
X <- df[, .SD, .SDcols = grep('pca_', colnames(df), value = TRUE)]
} else {
X <- df[, .SD, .SDcols = grep('month', colnames(df), value = TRUE):ncol(df)]
}
# Extract the target variable
y <- df$cpi_pct
# Subset the test data
X_test <- test_df[, .SD, .SDcols = colnames(X)]
X <- as.matrix(X)
X_test <- as.matrix(X_test)
y <- as.vector(y)
head(X)
gaussian_log_likelihood <- function(predictions, y) {
n <- length(y)
k <- ncol(predictions)
residuals <- y - predictions
sigma_sq <- apply(residuals^2, 2, sum) / (n - k)
log_likelihood <- -0.5 * (n * log(2 * pi * sigma_sq) + colSums(residuals^2) / sigma_sq)
return(log_likelihood)
}
lasso.bic <- function(x,y,extended=FALSE) {
#Select model in LASSO path with best BIC (using LASSO regression estimates)
#Input
# - y: vector with response variable
# - x: design matrix
# - extended: whether to use EBIC (Chen and Chen 2008) instead of BIC
#
#Output: list with the following elements
# - coef: LASSO-estimated regression coefficient with lambda set via BIC
# - ypred: predicted y
# - lambda.opt: optimal value of lambda
# - lambda: data.frame with bic and number of selected variables for each value of lambda
print(dim(x))
require(glmnet)
fit <- glmnet(x=x,y=y,family='gaussian',alpha=1)
pred <- cbind(1,x) %*% rbind(fit$a0,fit$beta)
# print(pred)
n <- length(y)
p <- colSums(fit$beta!=0) + 1
if (!extended){
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p
} else {
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p + 2*log(choose(ncol(x),p))
}
sel <- which.min(bic)
beta <- c(fit$a0[sel],fit$beta[,sel]); names(beta)[1]= 'Intercept'
ypred <- pred[,sel]
ans <- list(coef=beta,ypred=ypred,lambda.opt=fit$lambda[sel],lambda=data.frame(lambda=fit$lambda,bic=bic,nvars=p))
return(ans)
}
result_BIC <- lasso.bic(X, y)
result_EBIC <- lasso.bic(X, y, TRUE)
result_BIC$lambda[order(result_BIC$lambda$bic), ]
result_EBIC$lambda[order(result_EBIC$lambda$bic), ]
test_pred_BIC <- cbind(1,X_test) %*% result_BIC$coef
test_pred_EBIC <- cbind(1,X_test) %*% result_EBIC$coef
# Create a data frame with the predictions
predictions_df <- data.frame(pred = test_pred_BIC)
# Write the data frame to a CSV file
write.csv(predictions_df, paste("predictions/", pca_prefix, "lasso_bic.csv", sep = ""))
# Create a data frame with the predictions
predictions_df <- data.frame(pred = test_pred_EBIC)
# Write the data frame to a CSV file
write.csv(predictions_df, paste("predictions/", pca_prefix, "lasso_ebic.csv", sep = ""))
library(hdm)
library(glmnet)
library(ggplot2)
library(tidyverse)
library(HDCI)
library(gridExtra)
library(mombf) # Bayesian model selection and Bayesian model averaging
library(rstanarm)
mc.cores = parallel::detectCores()
# Define a function to calculate scores for different 'g' values
calculate_scores <- function(y_f, x_f, g_range) {
# Initialize an empty dataframe to store results
result_df <- data.frame(G = numeric(0), Score = numeric(0))
# Loop through the range of 'g' values
for (g in g_range) {
# Fit the model with the current 'g' value
fit <- modelSelection(y=y_f ,x=x_f, priorCoef=zellnerprior(taustd=g), priorDelta=modelbbprior(1,1))
post.prob = postProb(fit)
# Extract the score (the first element in the 'pp' column)
score <- post.prob$pp[1]
# Append the 'g' and score to the result dataframe
result_df <- rbind(result_df, data.frame(G = g, Score = score))
}
return(result_df)
}
g_cv <- 1/8.697490026177834e-05
g_ebic <- 1/3.219938e-04
print(g_cv)
# Define a range of 'g' values
g_range <- c(5, 50, 100, 500, 1000, 1200, 1500, 2500, 3000, 5000, 10000)
# g_range <- seq(0.03, 1, by = 0.03)
# Call the function to calculate scores for different 'g' values
result_dataframe <- calculate_scores(y_f = y, x_f = X, g_range)
knitr::opts_chunk$set(echo = TRUE)
# Set the flag for PCA
USE_PCA <- FALSE  # Change this to FALSE if not using PCA
library(glmnet)
library(caret)
library(data.table)
# Define PCA prefixes and suffixes
pca_prefix <- if (USE_PCA) 'pca_' else ''
pca_suffix <- if (USE_PCA) '_pca' else ''
# Read the CSV files
df <- fread(paste0('datasets/train_set', pca_suffix, '.csv'))
test_df <- fread(paste0('datasets/test_set', pca_suffix, '.csv'))
# Subset the data based on PCA flag
if (USE_PCA) {
X <- df[, .SD, .SDcols = grep('pca_', colnames(df), value = TRUE)]
} else {
X <- df[, .SD, .SDcols = grep('month', colnames(df), value = TRUE):ncol(df)]
}
df
df[:, 4:20]
df[, 5:ncol(df)]
knitr::opts_chunk$set(echo = TRUE)
# Set the flag for PCA
USE_PCA <- FALSE  # Change this to FALSE if not using PCA
library(glmnet)
library(caret)
library(data.table)
# Define PCA prefixes and suffixes
pca_prefix <- if (USE_PCA) 'pca_' else ''
pca_suffix <- if (USE_PCA) '_pca' else ''
# Read the CSV files
df <- fread(paste0('datasets/train_set', pca_suffix, '.csv'))
test_df <- fread(paste0('datasets/test_set', pca_suffix, '.csv'))
# Subset the data based on PCA flag
if (USE_PCA) {
X <- df[, 5:ncol(df)]
} else {
X <- df[, 4:ncol(df)]
}
# Extract the target variable
y <- df$cpi_pct
# Subset the test data
X_test <- test_df[, .SD, .SDcols = colnames(X)]
X <- as.matrix(X)
X_test <- as.matrix(X_test)
y <- as.vector(y)
head(X)
gaussian_log_likelihood <- function(predictions, y) {
n <- length(y)
k <- ncol(predictions)
residuals <- y - predictions
sigma_sq <- apply(residuals^2, 2, sum) / (n - k)
log_likelihood <- -0.5 * (n * log(2 * pi * sigma_sq) + colSums(residuals^2) / sigma_sq)
return(log_likelihood)
}
lasso.bic <- function(x,y,extended=FALSE) {
#Select model in LASSO path with best BIC (using LASSO regression estimates)
#Input
# - y: vector with response variable
# - x: design matrix
# - extended: whether to use EBIC (Chen and Chen 2008) instead of BIC
#
#Output: list with the following elements
# - coef: LASSO-estimated regression coefficient with lambda set via BIC
# - ypred: predicted y
# - lambda.opt: optimal value of lambda
# - lambda: data.frame with bic and number of selected variables for each value of lambda
print(dim(x))
require(glmnet)
fit <- glmnet(x=x,y=y,family='gaussian',alpha=1)
pred <- cbind(1,x) %*% rbind(fit$a0,fit$beta)
# print(pred)
n <- length(y)
p <- colSums(fit$beta!=0) + 1
if (!extended){
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p
} else {
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p + 2*log(choose(ncol(x),p))
}
sel <- which.min(bic)
beta <- c(fit$a0[sel],fit$beta[,sel]); names(beta)[1]= 'Intercept'
ypred <- pred[,sel]
ans <- list(coef=beta,ypred=ypred,lambda.opt=fit$lambda[sel],lambda=data.frame(lambda=fit$lambda,bic=bic,nvars=p))
return(ans)
}
result_BIC <- lasso.bic(X, y)
result_EBIC <- lasso.bic(X, y, TRUE)
result_BIC$lambda[order(result_BIC$lambda$bic), ]
result_EBIC$lambda[order(result_EBIC$lambda$bic), ]
test_pred_BIC <- cbind(1,X_test) %*% result_BIC$coef
test_pred_EBIC <- cbind(1,X_test) %*% result_EBIC$coef
# Create a data frame with the predictions
predictions_df <- data.frame(pred = test_pred_BIC)
# Write the data frame to a CSV file
write.csv(predictions_df, paste("predictions/", pca_prefix, "lasso_bic.csv", sep = ""))
# Create a data frame with the predictions
predictions_df <- data.frame(pred = test_pred_EBIC)
# Write the data frame to a CSV file
write.csv(predictions_df, paste("predictions/", pca_prefix, "lasso_ebic.csv", sep = ""))
library(hdm)
library(glmnet)
library(ggplot2)
library(tidyverse)
library(HDCI)
library(gridExtra)
library(mombf) # Bayesian model selection and Bayesian model averaging
library(rstanarm)
mc.cores = parallel::detectCores()
# Define a function to calculate scores for different 'g' values
calculate_scores <- function(y_f, x_f, g_range) {
# Initialize an empty dataframe to store results
result_df <- data.frame(G = numeric(0), Score = numeric(0))
# Loop through the range of 'g' values
for (g in g_range) {
# Fit the model with the current 'g' value
fit <- modelSelection(y=y_f ,x=x_f, priorCoef=zellnerprior(taustd=g), priorDelta=modelbbprior(1,1))
post.prob = postProb(fit)
# Extract the score (the first element in the 'pp' column)
score <- post.prob$pp[1]
# Append the 'g' and score to the result dataframe
result_df <- rbind(result_df, data.frame(G = g, Score = score))
}
return(result_df)
}
g_cv <- 1/8.697490026177834e-05
g_ebic <- 1/3.219938e-04
print(g_cv)
# Define a range of 'g' values
g_range <- c(5, 50, 100, 500, 1000, 1200, 1500, 2500, 3000, 5000, 10000)
# g_range <- seq(0.03, 1, by = 0.03)
# Call the function to calculate scores for different 'g' values
result_dataframe <- calculate_scores(y_f = y, x_f = X, g_range)
print(head(result_dataframe))
ggplot(result_dataframe, aes(x = G, y = Score)) +
geom_point() +
labs(x = "g", y = "Model Proba") +
ggtitle("Best Model_margpp vs. g")
g = 11000
margpp <- 0.5
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = g), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
ci.bayesreg <- coef(fit.bayesreg)[-c(1,nrow(coef(fit.bayesreg))),]
sel.bayesreg <- ci.bayesreg[,4] > 0.5
ci.bayesreg[,1:3] <- round(ci.bayesreg[,1:3], 3)
ci.bayesreg[,4] <- round(ci.bayesreg[,4], 4)
sorted_indices <- order(ci.bayesreg[, "margpp"], decreasing = TRUE)
# Sort the matrix by the specified column
ci.bayesreg <- ci.bayesreg[sorted_indices, ]
# Fill missing values with 0 using the fillna() function
ci.bayesreg <- ifelse(is.na(ci.bayesreg), 0, ci.bayesreg)
head(ci.bayesreg, 5)
tail(ci.bayesreg, 5)
sum(ci.bayesreg[, 'margpp'] > margpp)
bma_betas <- coef(fit.bayesreg)[, "estimate", drop = FALSE]
bma_betas <- bma_betas[-nrow(bma_betas), ]
print(dim(X_test))
print(dim(cbind(1, X_test)))
head(bma_betas)
test_pred_BMA <-cbind(1,X_test) %*% bma_betas
# Create a data frame with the predictions
predictions_df <- data.frame(pred = test_pred_BMA)
# Write the data frame to a CSV file
write.csv(predictions_df, paste("predictions/", pca_prefix, "bma.csv", sep = ""))
knitr::opts_chunk$set(echo = TRUE)
# Set the flag for PCA
USE_PCA <- TRUE  # Change this to FALSE if not using PCA
library(glmnet)
library(caret)
library(data.table)
# Define PCA prefixes and suffixes
pca_prefix <- if (USE_PCA) 'pca_' else ''
pca_suffix <- if (USE_PCA) '_pca' else ''
# Read the CSV files
df <- fread(paste0('datasets/train_set', pca_suffix, '.csv'))
test_df <- fread(paste0('datasets/test_set', pca_suffix, '.csv'))
# Subset the data based on PCA flag
if (USE_PCA) {
X <- df[, 5:ncol(df)]
} else {
X <- df[, 4:ncol(df)]
}
# Extract the target variable
y <- df$cpi_pct
# Subset the test data
X_test <- test_df[, .SD, .SDcols = colnames(X)]
X <- as.matrix(X)
X_test <- as.matrix(X_test)
y <- as.vector(y)
head(X)
gaussian_log_likelihood <- function(predictions, y) {
n <- length(y)
k <- ncol(predictions)
residuals <- y - predictions
sigma_sq <- apply(residuals^2, 2, sum) / (n - k)
log_likelihood <- -0.5 * (n * log(2 * pi * sigma_sq) + colSums(residuals^2) / sigma_sq)
return(log_likelihood)
}
lasso.bic <- function(x,y,extended=FALSE) {
#Select model in LASSO path with best BIC (using LASSO regression estimates)
#Input
# - y: vector with response variable
# - x: design matrix
# - extended: whether to use EBIC (Chen and Chen 2008) instead of BIC
#
#Output: list with the following elements
# - coef: LASSO-estimated regression coefficient with lambda set via BIC
# - ypred: predicted y
# - lambda.opt: optimal value of lambda
# - lambda: data.frame with bic and number of selected variables for each value of lambda
print(dim(x))
require(glmnet)
fit <- glmnet(x=x,y=y,family='gaussian',alpha=1)
pred <- cbind(1,x) %*% rbind(fit$a0,fit$beta)
# print(pred)
n <- length(y)
p <- colSums(fit$beta!=0) + 1
if (!extended){
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p
} else {
bic <- -2 * gaussian_log_likelihood(pred, y) + n*(log(2*pi)+1) + log(n)*p + 2*log(choose(ncol(x),p))
}
sel <- which.min(bic)
beta <- c(fit$a0[sel],fit$beta[,sel]); names(beta)[1]= 'Intercept'
ypred <- pred[,sel]
ans <- list(coef=beta,ypred=ypred,lambda.opt=fit$lambda[sel],lambda=data.frame(lambda=fit$lambda,bic=bic,nvars=p))
return(ans)
}
result_BIC <- lasso.bic(X, y)
result_EBIC <- lasso.bic(X, y, TRUE)
result_BIC$lambda[order(result_BIC$lambda$bic), ]
result_EBIC$lambda[order(result_EBIC$lambda$bic), ]
test_pred_BIC <- cbind(1,X_test) %*% result_BIC$coef
test_pred_EBIC <- cbind(1,X_test) %*% result_EBIC$coef
# Create a data frame with the predictions
predictions_df <- data.frame(pred = test_pred_BIC)
# Write the data frame to a CSV file
write.csv(predictions_df, paste("predictions/", pca_prefix, "lasso_bic.csv", sep = ""))
# Create a data frame with the predictions
predictions_df <- data.frame(pred = test_pred_EBIC)
# Write the data frame to a CSV file
write.csv(predictions_df, paste("predictions/", pca_prefix, "lasso_ebic.csv", sep = ""))
library(hdm)
library(glmnet)
library(ggplot2)
library(tidyverse)
library(HDCI)
library(gridExtra)
library(mombf) # Bayesian model selection and Bayesian model averaging
library(rstanarm)
mc.cores = parallel::detectCores()
# Define a function to calculate scores for different 'g' values
calculate_scores <- function(y_f, x_f, g_range) {
# Initialize an empty dataframe to store results
result_df <- data.frame(G = numeric(0), Score = numeric(0))
# Loop through the range of 'g' values
for (g in g_range) {
# Fit the model with the current 'g' value
fit <- modelSelection(y=y_f ,x=x_f, priorCoef=zellnerprior(taustd=g), priorDelta=modelbbprior(1,1))
post.prob = postProb(fit)
# Extract the score (the first element in the 'pp' column)
score <- post.prob$pp[1]
# Append the 'g' and score to the result dataframe
result_df <- rbind(result_df, data.frame(G = g, Score = score))
}
return(result_df)
}
g_cv <- 1/8.697490026177834e-05
g_ebic <- 1/3.219938e-04
print(g_cv)
# Define a range of 'g' values
g_range <- c(5, 50, 100, 500, 1000, 1200, 1500, 2500, 3000, 5000, 10000)
# g_range <- seq(0.03, 1, by = 0.03)
# Call the function to calculate scores for different 'g' values
result_dataframe <- calculate_scores(y_f = y, x_f = X, g_range)
print(head(result_dataframe))
ggplot(result_dataframe, aes(x = G, y = Score)) +
geom_point() +
labs(x = "g", y = "Model Proba") +
ggtitle("Best Model_margpp vs. g")
g = 11000
margpp <- 0.5
fit.bayesreg <- modelSelection(y=y, x=X, priorCoef=zellnerprior(taustd = g), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
ci.bayesreg <- coef(fit.bayesreg)[-c(1,nrow(coef(fit.bayesreg))),]
sel.bayesreg <- ci.bayesreg[,4] > 0.5
ci.bayesreg[,1:3] <- round(ci.bayesreg[,1:3], 3)
ci.bayesreg[,4] <- round(ci.bayesreg[,4], 4)
sorted_indices <- order(ci.bayesreg[, "margpp"], decreasing = TRUE)
# Sort the matrix by the specified column
ci.bayesreg <- ci.bayesreg[sorted_indices, ]
# Fill missing values with 0 using the fillna() function
ci.bayesreg <- ifelse(is.na(ci.bayesreg), 0, ci.bayesreg)
head(ci.bayesreg, 5)
tail(ci.bayesreg, 5)
sum(ci.bayesreg[, 'margpp'] > margpp)
bma_betas <- coef(fit.bayesreg)[, "estimate", drop = FALSE]
bma_betas <- bma_betas[-nrow(bma_betas), ]
print(dim(X_test))
print(dim(cbind(1, X_test)))
head(bma_betas)
test_pred_BMA <-cbind(1,X_test) %*% bma_betas
# Create a data frame with the predictions
predictions_df <- data.frame(pred = test_pred_BMA)
# Write the data frame to a CSV file
write.csv(predictions_df, paste("predictions/", pca_prefix, "bma.csv", sep = ""))
View(predictions_df)
